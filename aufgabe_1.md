# Datenspezifische Aufgaben

Ein großangelegtes, DFG-geförderts Forschungsprojekt produziert eine Menge wissenschaftlicher Daten unterschiedlichster Art. Im Fall meiner Arbeit im Schwerpunkt-Programm (SPP) ["Entangeld Africa"](https://www.dainst.blog/entangled-africa/en/home/) der Kommission für Archäologie Außereuropäischer Kulturen (KAAK) des Deutschen Archäologischen Instituts (DAI) handelt es sich in erster Linie um archäologische, historische und naturwissenschaftliche Forschungsdaten.

## Archäologische Forschungsdaten
Die verschiedenen inhaltlichen Teilprojekte des SPP teilen die aus ihrer Forschung resultierenden Daten mit einem speziell für die Datenverarbeitung verantwortlichen [Teilprojekt](https://www.dainst.blog/entangled-africa/en/p11-data-management-en/). Dort werden die Forschungsdaten gegebenenfalls aufbereitet und über die verschiedenen Online-Plattformen des DAI, der [iDAI.world](https://idai.world/), der Wissenschaftscommunity sowie der interessierten Öffentlichkeit zugänglich gemacht.

Teil meines Arbeitsalltags ist unter anderem die Pflege chronologischer Forschungsdaten für die Regionen des SPP. Die dafür genutzte DAI-Plattform [ChronOntology](https://chronontology.dainst.org/) stellt chronologische Einteilungen verschiedener Regionen geordnet nach unterschiedlichen Kategorien bereit. Die jeweiligen Einträge beinhalten dabei kurze Definitionen und Beschreibungen, interaktive Karteneinbindungen sowie ausgewählte Literaturverweise. Die chronologischen Perioden können einer der Kategorien *politisch*, *materielle Kultur*, *geologisch* oder *kulturell* angehören. Konkret arbeitet unsere Team aktuell an den Chronologien der Nilregionen. Hauptziel ist es, eine möglichst allgemeingültige Definitionen der verschiedenen Epochen und Perioden zu erarbeitenk, die im besten Fall von den Forschenden unterschiedlicher altertumswissenschaftlicher Disziplinen übernommen werden. Die daraus angestrebte Vereinheitlichung der aktuell noch zum Teil sehr verschiedenen definierten Epochen- bzw. Periodengrenzen soll auch zu einer besseren Vergleichbarkeit und größerem Anteil an interdisziplinären Forschungsvorhaben führen. Dafür muss in erster Linie archäologische und historische Recherche betrieben werden, um die allgemein anerkannten Aspekte, die die jeweilige Epoche definieren, herauszuarbeiten. Gerade bei wissenschaftlich kontrovers disskutierten Themen kann dies zeit- und arbeitsintensiv sein. In solch einem Fall sollten die differierenden Meinungen bestmöglichst abgebildet werden.

Ein paar **Beispieldatensätze** können helfen die Unterschiede der chronologischen Kategorien besser zu verstehen:

- Die [**Meroitische Zeit**]( http://chronontology.dainst.org/period/jm3xupRlfnKp ) gehört der Kategorie *politisch* an, da es sich hierbei um eine Herrschafts- bzw. Regierungsepoche handelt, welche sich auf eine dynastische Herrschaftsfolge stützt. Es ist also eine gesellschafts-politisch konstruierte Ordnung.
- Die [**Early Aksumite Phase**]( http://chronontology.dainst.org/period/WduFs7QqO0FA ) gehört der Kategorie *materielle Kultur* mit der Konkretisierung *Keramikstil* an, da sich diese zeitliche Einteilung durch das Auftreten eines bestimmten Keramiktypens definiert.
- Bei der sehr allgemeinen Epoche [**Early Holocene**]( http://chronontology.dainst.org/period/iFSXg6L89xVF ) handelt es sich um einen *geologischen* Eintrag, da diese erdgeschichtlichen Perioden in erster Linie durch geologische und/oder klimatische Eigenheiten definiert sind.[^1]


## Arbeitsabläufe/Prozesse
Das Einpflegen der chronologischen Forschungsdaten wird mithilfe einer .csv-Tabelle vorgenommen. Die meisten Kolleg:innen bearbeiten diese mit Excel oder OpenOffice. In der Tabelle erhält jeder Datensatz eine Zeile, in den Spalten werden dann die jeweiligen Attribute eingetragen. Die verschiedenen Felder enthalten dabei sehr unterschiedliche Einträge:

- Das Namensfeld enthält eine Liste gängiger Bezeichnungen der Periode in verschiedenen Sprachen, angezeigt durch einen kurzen Sprachcode ("en", "de" usw.) 
Bzgl. des Sprachcodes könnte geprüft werden, inwiefern die Codes für die Darstellung von Sprachnamen normiert sind, z. B. gem. ISO 639-1. Ggf. sollte eine Vereinheitlichung stattfinden.
- Manche Felder enthalten nur eine Datensatz-ID, die später eine automatische Verlinkung einfügt. Verweist der gerade bearbeitete Datensatz auf einen anderen, der ebenfalls noch erstellt wird, wird diese ID bis zur Einspeisung in die Datenbank durch einen Platzhalter ersetzt. Dies betrifft vor allem die ordnenden, periodenbezogenen Kategorien, wie *folgt auf*, *wird gefolgt von*, *ist Teil von* usw.
- Einige Felder sind Freitext-Felder. So *Definition* und *Beschreibung*. In diesen wird die Periode eingeordnet und die wichtigsten historischen, kulturellen oder archäologischen Eigenschaften kurz beschrieben. Es gibt keine feste Zeichenbegrenzung.
- Es gibt Felder für Literatureinträge sowie eines, das die genutzte Literatur mit den entsprechenden Datensätzen im DAI-eigenen Online-Katalog [**Zenon**](https://zenon.dainst.org/) verbindet.
- Es gibt ein Feld für die zeitlichen Grenzen im Freitext sowie eines, das die Daten maschinenlesbar abbildet.

Über die fertigen (Teil)Tabellen läuft, nachdem diese noch aus dem -xlsx bzw. .ods Format in .csv umgewandelt wurden, anschließend ein Script, durch welches die Daten ins JSON-Format konvertiert werden. Dieses bildet die Grundlage der online zugänglichen Datenbank und wird auch für Anfragen über die API-Schnittstelle der Website genutzt.

Diejeinigen Mitarbeiter:innen, die die Datenkuratierung anhand der Tabelle vornehmen, haben mit allen folgenden Schritten (Script, JSON etc.) allerdings nichts zu tun. Dies liegt in Händen der IT-Abteilung. Der Datenkuratierung hingegen geht in vielen Fällen intensive Recherche und Überprüfung vorhandener Informationen voraus, weshalb hier vermehrt inhaltlich-fachliche Kenntnisse (in diesem Fall v.a. der Archäologie sowie der Ur- und Frühgeschichte) gefragt sind und weniger informationtechnische Fähigkeiten.

## Verbesserungsideen
Der geschilderte Workflow ließe sich an verschiedenen Punkten durch it-technische Anpassungen bzw. Verbesserungen sicherlich effektiver gestalten.

Ein grundlegener Aspekt, bevor es um konkrete Details im Arbeitsablauf geht, ist die verwendete Software. Da viele Mitarbeiter:innen keine besondere Ausbildung in der Nutzung verschiedener Betriebssysteme, Nutzeroberflächen und Software haben, ist eine weit verbreitete "Standard"-Software nötig. Doch hier fehlt die Vereinheitlichung. Die meisten nutzen Excel, andere ein freies Office-Programm (z.B. LibreOffice Calc). Die Kompabilität zwischen beiden ist durchaus verbesserungswürdig. Keines der Programme ist meines Erachtens wirklich gut für die Arbeit geeignet, weil sie bei ausführlichen Tabellen und längeren Freitext-Anteilen unübersichtlich werden und Teils merklich an Geschwindigkeit einbüßen. Da eine Schulung der Mitarbeitenden in direkter Bearbeitung von .csv-Dateien durch die Nutzung von Python sowie mithilfe angepasster Editoren o.Ä. von Seiten der Institution nicht vorgesehen ist, wäre zumindest eine einheitlich genutzte Software mit professionell erstellten Dokumentenvorlagen von Vorteil. So lassen sich Kompabilitätsprobleme und unterschiedliche formatierte Dateien schon einmal vermeiden. Persönlich plädiere ich natürlich für eine freie Software wie LibreOffice, die meiner Erfahrung nach auch in der Bedienung Excel vorzuziehen ist.

Konkrete softwaregestützte Verbesserungsmöglichkeiten im Workflow sehe ich an folgenden Stellen:

- **Automatische Literaturerkennung**: Sofern ein einheitliches Bibliographieformat für die Datenkuratierung gefunden werden kann (z.B. RIS, BibTex etc.) sollte sich ohne allzu viel Aufwand ein Skript o.Ä. schreiben lassen, dass die in den Datensätzen verwendete Literatur automatisch mit den Einträgen in Zenon verknüpft und das Ergebnis in das genutzte JSON-Format überführt. Auch DOIs usw. ließen sich auf diese Weise automatisch einbinden. Dies würde das oben genannte, manuell erstellte Feld in der Tabelle, durch welches die Literaturangaben mit den Zenon-Einträgen verknüpft werden, überflüssig machen. Zur Zeit wird alles händisch verknüpft und jede:r nutzt den Zitierstil, den er/sie für passend hält.
- **Ersetzen der Platzhalter-IDs**: Derzeit werden die Platzhalter-IDs in fertigen Tabellen durch *Suchen & Ersetzen* ausgetauscht. Das reslutiert in einem großen *Copy & Paste* Arbeitsgang, bei dem alle Nacheinander manuell durchgegangen werden. Durch eine stärkere Vereinheitlichung der Platzhalter ließe sich auch dieser Austausch sicherlich recht einfach durch ein Skript ersetzen.
- **Zeitliche Epochengrenzen**: Da es schon ein Feld für eine maschinenlesbare Angabe der jeweiligen Epochengrenzen gibt, sollte sich das zusätzliche freihändische Feld leicht durch ein Skript ersetzen lassen, das diese Daten einfach aus ersterem zieht.
- **Kontrolliertes Vokabular**: Zwar besteht die Möglichkeit, Datensätze zu verschlagworten, doch exitiert dazu keine Vorgabe eines kontrollierten Vokabulars bwz. eines Thesaurus. Dabei bietet die iDAI.world selbst sogar derartige [Portale](http://thesauri.dainst.org/de.html) (die allerdings zum Teil noch in der Entwicklung sind). Diese sollten sich ohne Probleme über eine Schnittstelle in die Datenkuratierung einbinden lassen. Im besten Fall sogar mitsamt einer Autoversollständigung oder/und einem Drop-Down-Menü im entsprechenden Feld, so dass auch it-technisch ungeübtere Mitarbeitende auf die richtigen Schlagworte hingewiesen werden und Falscheingaben ausbleiben. Dies führt in der fertigen Datenbank zu deutlich mehr Möglichkeiten bezüglich der Gestaltung spezifischer Suchanfragen und unterstützt auch automatische Queries über die API des Portals.
Evtl. sollte neben der Indexierung des kontrollierten Vokabulars aus dem iDAI.world-Potal geprüft werden, inwiefern Normdateien/authority files eingebunden werden kön-nen/sollen (z. B. über die LOD-API für die GND > lobid-gnd).
Sicherlich gibt es vor allem in den Backend-Prozessen der Datenkonvertierung noch mehr Arbeitsschritte, die durch eine Überarbeitung den Gesamtablauf effizienter gestalten können. Als Mitarbeitet am Frontend, heisst in diesem Fall dem Tabellenkalkulationsprogramm, kann ich derartige Schritte aber bestenfalls vermuten. An dieser Seite des Workflows setzt auch einfach der Software-Zwang natürliche Grenzen, da es viele Optionen/Möglichkeiten gibt, die sich mit Excel/LibreOffice schlicht nicht vernünftig umsetzen lassen. Geschuldet ist dies wohl auch etwas der bereits oben geschilderten Tatsache, dass die Mitarbeitenden in der Datenkuratierung in erster Linie über eine archäologisch/frühgeschichtliche Ausbildung verfügen müssen und weniger über eine informationstechnische.
Künftig könnte die Anbindung des Datenbestands an weitere Portale angestrebt werden, z. B. an das Metasuchportal „AfricanStudiesLibrary“, das derzeit vom Fachinformationsdienst Afrikastudien implementiert wird.
[^1]: Trotzdem unterscheiden sich auch diese weiter gefassten Perioden von Region zu Region etwas.
